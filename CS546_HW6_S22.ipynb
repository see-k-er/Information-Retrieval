{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS546_HW6_S22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKC3O3QiIi4"
      },
      "source": [
        "#COMPSCI 546: Applied Information Retrieval - Spring 2022 ([website](https://groups.cs.umass.edu/zamani/compsci-546-applied-information-retrieval-spring-2022/))\n",
        "##Assignment 6: Clustering (Total : 100 points)\n",
        "\n",
        "**Description**\n",
        "\n",
        "This is a coding assignment where you will implement the k-means clustering algorithm to cluster passages. Basic proficiency in Python is recommended.  \n",
        "\n",
        "**Instructions**\n",
        "\n",
        "* To start working on the assignment, you would first need to save the notebook to your local Google Drive. For this purpose, you can click on *Copy to Drive* button. You can alternatively click the *Share* button located at the top right corner and click on *Copy Link* under *Get Link* to get a link and copy this notebook to your Google Drive.  \n",
        "\n",
        "*   For questions with descriptive answers, please replace the text in the cell which states \"Enter your answer here!\" with your answer. If you are using mathematical notation in your answers, please define the variables.\n",
        "*   You should implement all the functions yourself and should not use a library or tool for the computation.\n",
        "*   For coding questions, you can add code where it says \"enter code here\" and execute the cell to print the output.\n",
        "* To create the final pdf submission file, execute *Runtime->RunAll* from the menu to re-execute all the cells and then generate a PDF using *File->Print->Save as PDF*. Make sure that the generated PDF contains all the codes and printed outputs before submission.\n",
        "To create the final python submission file, click on File->Download .py.\n",
        "\n",
        "\n",
        "**Submission Details**\n",
        "\n",
        "* Due data: Apr. 12, 2022 at 11:59 PM (EDT).\n",
        "* The final PDF and python file must be uploaded on Moodle.\n",
        "* After copying this notebook to your Google Drive, please paste a link to it below. Use the same process given above to generate a link. ***You will not recieve any credit if you don't paste the link!*** Make sure we can access the file. \n",
        "***LINK: *https://colab.research.google.com/drive/1BWlA9dS3IKgZRnhJkqT_DjkegfKf7idV?usp=sharing***\n",
        "\n",
        "**Academic Honesty**\n",
        "\n",
        "Please follow the guidelines under the *Collaboration and Help* section of the course website.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVqvXU6ijXi"
      },
      "source": [
        "# Download input files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMU5d1a-jCG-"
      },
      "source": [
        "Please execute the cell below to download the input files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM1igiBMcIB8"
      },
      "source": [
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "\n",
        "download = drive.CreateFile({'id': '1NR5jMPJAJHD3flGEbckMr8WerfPtLtZ2'})\n",
        "download.GetContentFile('HW06.zip')\n",
        "\n",
        "with zipfile.ZipFile('HW06.zip', 'r') as zip_file:\n",
        "    zip_file.extractall('./')\n",
        "os.remove('HW06.zip')\n",
        "# We will use hw05 as our working directory\n",
        "os.chdir('HW06')\n",
        "\n",
        "#Setting the input file\n",
        "passage = \"passages.tok.clean_kstem\"\n",
        "init_centroid_file = \"centroid.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L84x5CFj9Z-L"
      },
      "source": [
        "# 1 : Initial Data Setup (25 points)\n",
        "\n",
        "We use collection from the ANTIQUE  [https://arxiv.org/pdf/1905.08957.pdf] dataset for this assignment. As described in the previous assignments, this is a passage retrieval dataset. \n",
        "\n",
        "The description of the input files provided for this assignment is given below.\n",
        "\n",
        "**Collection file**\n",
        "\n",
        "Each row of the file consists of the following information:\n",
        "\n",
        "*passage_id  passage_text*\n",
        "\n",
        "The id and text information is tab separated. The passage text has been pre-processed to remove punctutation, tokenised and stemmed using the Krovetz stemmer. The terms in the passage text can be accessed by splitting the text based on space.\n",
        "\n",
        "**Centroid vector values**\n",
        "\n",
        "Each row is a tab separated entry where the first column is cluster id and the second column is the vector which is used to initialize the cluster centroid in the k-means Algorithm. \n",
        "\n",
        "In the cell below, collection information as vocababulary size, document frequency and passage count and initial centroid vector values have been given. This can be used in subsequent cells for computation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7pEmMOeaoMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf9e6de-9d67-4f55-f501-0faba307028f"
      },
      "source": [
        "''' \n",
        "In this function, load the initial centroid values for k clusters.  \n",
        "Return Variables: \n",
        "init_centroid_vec - array where each element corresponds to the centroid vector. \n",
        "'''\n",
        "\n",
        "# load initial centroid values\n",
        "def load_centroid_init(init_centroid_file):\n",
        "  init_centroid_vec = []\n",
        "  for line in open(init_centroid_file):\n",
        "   row = line.strip().split('\\t')\n",
        "   init_centroid_vec.append(np.fromstring(row[1][1:-1],sep=',').astype(float))\n",
        "  return init_centroid_vec \n",
        "\n",
        "\n",
        "''' \n",
        "In this function, iterate through the input passage and load the data  \n",
        "Return Variables: \n",
        "vocab - dict mapping word to an integer [0,len(vocab)]\n",
        "df - dict mapping word to document frquency\n",
        "num_passages - total number of passages in the input collection\n",
        "docs-dict mapping each passage id to unique integer between [0,len(num_passages)]\n",
        "'''\n",
        "# load vocabulary\n",
        "def load_vocab(passage):\n",
        "  df = {}\n",
        "  vocab = {}\n",
        "  count = 0\n",
        "  doc_count = 0\n",
        "  num_passages = 0\n",
        "  docs = {}\n",
        "  for line in open(passage):  \n",
        "   row = line.strip().split('\\t')\n",
        "   docs[row[0]] = doc_count\n",
        "   doc_count+=1\n",
        "   terms = row[1].split(' ')\n",
        "   num_passages+=1\n",
        "   for word in set(terms):\n",
        "    if word not in df:\n",
        "      df[word]=0\n",
        "    df[word]+=1\n",
        "   for word in terms:  \n",
        "    if word not in vocab: \n",
        "      vocab[word]=count\n",
        "      count+=1 \n",
        "\n",
        "  return vocab,df,num_passages,docs     \n",
        "\n",
        "init_centroid_vec = load_centroid_init(init_centroid_file)\n",
        "vocab, df, num_passages, docs = load_vocab(passage)\n",
        "\n",
        "print('Vocabulary Size : {0}'.format(len(vocab)))\n",
        "print('Total Number of Passages: {0}'.format(num_passages))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size : 3630\n",
            "Total Number of Passages: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq54VT8zcMjr"
      },
      "source": [
        "**Passage Representation**\n",
        "\n",
        "In order to cluster passages, each passage has to be converted into a vector representation. For this assignment, we choose a $tf{\\text -}idf$ representation. Every passage is represented by a vector with dimensionality equal to vocab_size. Each dimension in the vector corresponds to a word. The value of each dimension of the vector is the $tf{\\text -}idf$ value of the word in that passage. These vectors can be stacked to create a matrix where each row of the matrix corresponds to a passage. \n",
        "\n",
        "Let the vocab_size be $n$ and num_passages be $m$.\n",
        "\n",
        "In the cell below, implement the following:\n",
        "\n",
        "1) Create an input data matrix $P = m \\times n$  where $P_{i,j}=tf{\\text -}idf(i,j)$. The definition of the $tf{\\text -}idf(i,j)$ formulation is given below. \n",
        "\n",
        "$$ tf{\\text -}idf(i,j) = ln(1+count(i,j)) ln(1 + \\frac{|C|}{df(j)}) $$\n",
        "\n",
        "\n",
        "$tf{\\text -}idf(i,j)$: the  $tf{\\text -}idf$ value of the word corresponding to integer $j$ and passage corresponding to integer $i$ as defined in vocab and docs data structures, respectively. \n",
        "\n",
        "$count(i,j)$ = number of times word corresponding to integer $j$ occurs in passage corresponding to integer $i$.\n",
        "\n",
        "$df(j)$ - The document frequency of word corresponding to integer $j$ (i.e., the number of documents that contain the $j^{th}$ word.\n",
        "\n",
        "$|C|$ - Total number of passages in the collection.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fIEB5l56-o-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51aa1dda-a72e-463f-bca6-6544c2d85e8d"
      },
      "source": [
        "from collections import Counter\n",
        "''' \n",
        "In this function, create input matrix  \n",
        "Return Variables: \n",
        "pass_matrix - matrix of shape m by n where each row corresponds to a passage and\n",
        "each column to a word\n",
        "'''\n",
        "def create_input_matrix(passage,df,num_passages,vocab):\n",
        "    #enter code here\n",
        "    pass_matrix = []\n",
        "    for line in open(passage):  \n",
        "      row = line.strip().split('\\t')\n",
        "      words = row[1].split(' ')\n",
        "      dict_count = Counter(words)\n",
        "      vector = [0] * len(vocab)\n",
        "      for k,v in dict_count.items():\n",
        "        ipos = vocab[k]\n",
        "        tfidf = np.log(1 + v) * np.log(1 + (num_passages/df[k]))\n",
        "        vector[ipos] = tfidf  \n",
        "      pass_matrix.append(vector)\n",
        "    return pass_matrix\n",
        "\n",
        "pass_matrix =  create_input_matrix(passage,df,num_passages,vocab)   \n",
        "\n",
        "print('Shape of the input matrix : {0}'.format(np.shape(pass_matrix)))  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the input matrix : (500, 3630)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V1g6gnbGrtR"
      },
      "source": [
        "# 2 :  k-means Algorithm (35 points)\n",
        "\n",
        "In this section you must implement the k-means clustering algorithm. The K-means function is called with $k=3$ ($k$ is the number of clusters) and the initial centroids have been set for you, the distance metric to be used is the squared Euclidean distance. Execute the algorithm for 30 iterations. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6Sssau6MO-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66909c66-61ac-4db2-8cc3-55f8ee1b017b"
      },
      "source": [
        "from collections import defaultdict\n",
        "# centroid is a matrix instantiated with initial centroid values\n",
        "''' \n",
        "In this function, centroid is instantiated with initial centroid values\n",
        "Here we assume for k=3, the cluster ids are {0,1,2}   \n",
        "Return Variables: \n",
        "centroid-matrix where rows correspond to clusters and columns correspond to words. \n",
        "'''\n",
        "def  init_centers(vocab, k, init_centroid_vec):\n",
        "  vocab_size = len(vocab)\n",
        "  centroid = np.zeros((k, vocab_size), dtype='float')\n",
        "  for i in range(len(init_centroid_vec)):\n",
        "     centroid[i] = init_centroid_vec[i]\n",
        "  return centroid   \n",
        "\n",
        "''' \n",
        "In this function, implement kmeans algorithm   \n",
        "Return Variables: \n",
        "final_cluster_assignment - array with cluster id values indexed by the passageid\n",
        " (integer mappings in docs)\n",
        "num_passage_cluster_final - dict with cluster-id and key and number of elements \n",
        "in the cluster as value\n",
        "'''\n",
        "def kmeans(num_passages, pass_matrix, centroid, k):\n",
        "   # enter code here\n",
        "   n = 30 \n",
        "   while (n>0):  \n",
        "      c = 0\n",
        "      final_dict = {}\n",
        "      final_cluster_assignment = []\n",
        "      for num in range(k):\n",
        "        final_dict[num] = []\n",
        "      for i in pass_matrix:\n",
        "        tot_dist = []\n",
        "        for j in centroid:\n",
        "          a = np.array(i)\n",
        "          b = np.array(j)\n",
        "          dist = np.square(np.sum((a-b)**2)) #to find the euclidean distance\n",
        "          tot_dist.append(dist)\n",
        "        centr = np.where(tot_dist == np.amin(np.array(tot_dist)))[0]#min distance\n",
        "        final_dict[int(centr)].append(c)\n",
        "        final_cluster_assignment.append(int(centr))\n",
        "        c += 1\n",
        "\n",
        "      #recalculating the centroid\n",
        "      for key, v in final_dict.items():\n",
        "        a = []\n",
        "        for pid in v:\n",
        "          row = pass_matrix[pid]\n",
        "          a.append(row)\n",
        "        #new centroid values\n",
        "        numpyA = np.array(a)\n",
        "        centroid[key] = numpyA.mean(axis=0)\n",
        "      n -= 1\n",
        "      num_passage_cluster_final = defaultdict()\n",
        "\n",
        "      for k1, v1 in final_dict.items():\n",
        "        num_passage_cluster_final[k1] = len(v1)\n",
        "\n",
        "   return final_cluster_assignment,num_passage_cluster_final \n",
        "\n",
        "centroid = init_centers(vocab, 3, init_centroid_vec)\n",
        "final_cluster_assignment, num_passage_cluster_final = kmeans(num_passages, \n",
        "                                                    pass_matrix, centroid, 3)\n",
        "\n",
        "print('Number of initial centroids : {0}'.format(len(centroid)) )\n",
        "\n",
        "# print number of elements in each cluster\n",
        "''' \n",
        " Hint: The values lies within the interval [360,370] for the largest cluster, \n",
        " [120,130] for the second largest cluster and [0,10] for the smallest cluster. \n",
        "'''\n",
        "for cid,cnum in num_passage_cluster_final.items():\n",
        "   print(cid, cnum)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of initial centroids : 3\n",
            "0 363\n",
            "1 128\n",
            "2 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pyCSMB1MOaA"
      },
      "source": [
        "# 3 :  Evaluation (30 points)\n",
        "\n",
        "In this section, you must implement two evaluation metrics: \n",
        "\n",
        "1) IntraCluster Similarity Metric - Average Diameter \n",
        "Distance of each cluster $S$\n",
        "\n",
        "$$Sim(S) = \\frac{1}{|S| (|S|-1)} \\sum_{x,y \\in S,x \\neq y} dist(x,y)$$\n",
        "\n",
        "$|S|$ - number of passages assigned to cluster $S$\n",
        "\n",
        "$dist(x,y)$ - the squared euclidean distance between passages $x$ and $y$.\n",
        "\n",
        "\n",
        "2) Intercluster Similarity Metric - \n",
        "Average Linkage Distance between a pair of clusters $S,T$\n",
        "\n",
        "$$Sim(S,T) = \\frac{1}{|S| |T|} \\sum_{x \\in S,y \\in T} dist(x,y)$$\n",
        "\n",
        "$|S|$ - number of passages assigned to cluster $S$\n",
        "\n",
        "$|T|$ - number of passages assigned to cluster $T$\n",
        "\n",
        "$dist(x,y)$ - the squared euclidean distance between passages $x$ and $y$.\n",
        "\n",
        "\n",
        "You have to calculate and print out the IntraCluster Similarity Metric for every cluster and Intercluster Similarity Metric for every cluster pair. This is illustrated below with examples. \n",
        "\n",
        "For IntraCluster metric, Average Diameter Distance for cluster (a, b, c),\n",
        "the value can be calculated as follows :$\\frac{dist(a, b)+dist(a, c)+dist(b, a)+dist(b, c)+ dist(c, a)+ dist(c, b)}{3*2} $ \n",
        "\n",
        "For InterCluster metric, Average Linkage Distance for clusters (a, b, c) and (d, e) , the value can be calculated as follows:  $\\frac{dist(a, d)+dist(a, e)+ dist(b, d)+ dist(b, e)+ dist (c, d)+ dist(c, e)}{3*2}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ8S-4ybMXgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fbcded-3f86-4483-e6f2-d3d516a3c127"
      },
      "source": [
        "from typing import KeysView\n",
        "''' \n",
        "In this function, implement intracluster similarity metric Average Diameter \n",
        "Distance   \n",
        "Return Variables: \n",
        "sim_score - dict with metric score corresponding to each of the three clusters\n",
        "'''\n",
        "\n",
        "def average_diameter_dist(num_passage_cluster_final,final_cluster_assignment,\n",
        "                          pass_matrix):\n",
        "    # enter code here\n",
        "    sim_score = defaultdict()\n",
        "    for key,v in num_passage_cluster_final.items():\n",
        "      dist_sum = 0\n",
        "      opmatrix = []\n",
        "      for i in range(0,len(final_cluster_assignment)):\n",
        "        if final_cluster_assignment[i] == key:\n",
        "          opmatrix.append(pass_matrix[i])\n",
        "\n",
        "      for i in range(0, len(opmatrix)):\n",
        "        for j in range(0, len(opmatrix)):\n",
        "          if i != j:\n",
        "            a = np.array([opmatrix[i]])\n",
        "            b = np.array([opmatrix[j]])\n",
        "            dist_sum += np.sum(np.square((a-b)))\n",
        "      sim = dist_sum/(v*(v-1))\n",
        "      sim_score[key] = sim\n",
        "    return sim_score\n",
        "\n",
        "''' \n",
        "In this function, implement intercluster similarity metric Average Linkage Distance   \n",
        "Return Variables: \n",
        "sim_score - dict with metric score corresponding to each pair of clusters\n",
        "'''\n",
        "def average_linkage_dist(num_passage_cluster_final,final_cluster_assignment,\n",
        "                         pass_matrix):\n",
        "    # enter code here\n",
        "    #num of cluster pairs\n",
        "    sim_score = defaultdict(float)\n",
        "    pairs = []\n",
        "    cluster_pass_matrix = defaultdict(list)\n",
        "    for i in range(0, len(num_passage_cluster_final)):\n",
        "      for j in range(i+1, len(num_passage_cluster_final)):\n",
        "        pairs.append((i,j))\n",
        "      for x in range(0, len(final_cluster_assignment)): #setting up pass_matrix for each cluster ID\n",
        "        if final_cluster_assignment[x] == i:\n",
        "          cluster_pass_matrix[i].append(pass_matrix[x])\n",
        "\n",
        "    for a in pairs:\n",
        "      first = a[0]\n",
        "      secon = a[1]\n",
        "      dist_sum = 0\n",
        "      for fir in range(0, num_passage_cluster_final[first]):\n",
        "        a = np.array([cluster_pass_matrix[first][fir]])\n",
        "        for sec in range(0, num_passage_cluster_final[secon]):        \n",
        "          b = np.array([cluster_pass_matrix[secon][sec]])\n",
        "          dist_sum += np.sum(np.square((a-b)))\n",
        "      p1 = num_passage_cluster_final[first]\n",
        "      p2 = num_passage_cluster_final[secon]\n",
        "      sim = dist_sum/(p1*p2)\n",
        "      sim_score[(first, secon)] = sim\n",
        "\n",
        "    return sim_score\n",
        "\n",
        "intra_score  = average_diameter_dist(num_passage_cluster_final,\n",
        "                                     final_cluster_assignment,pass_matrix) \n",
        "inter_score =  average_linkage_dist(num_passage_cluster_final,\n",
        "                                    final_cluster_assignment,pass_matrix)  \n",
        "\n",
        "# print intracluster scores for very cluster\n",
        "'''\n",
        "Hint: The value lies within the intervals [310,320] for the largest cluster,\n",
        "[1135,1145] for the second largest cluster and [3860,3870] for the smallest cluster.\n",
        "'''\n",
        "for cid,score in intra_score.items():\n",
        "   print(cid, score) \n",
        "\n",
        "# print intercluster scores for every pair of cluster\n",
        "'''\n",
        "Hint: The value lies within the intervals [740,750] between largest and second \n",
        "largest clusters.\n",
        "[2260,2270] between largest and smallest clusters and \n",
        "[2590,2600] between second largest and smallest cluster\n",
        "'''\n",
        "for cid_pair,score in inter_score.items():\n",
        "   print(cid_pair, score)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 319.1771238766518\n",
            "1 1140.0375066528834\n",
            "2 3867.7247873603037\n",
            "(0, 1) 744.3454376966359\n",
            "(0, 2) 2268.718168712698\n",
            "(1, 2) 2598.1074296167694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKx5wv8-t6IG"
      },
      "source": [
        "# 4 : k-Means Clustering (10 points)\n",
        "\n",
        "**Question 4.1:** For this assignment, we used a predefined number of clusters, $k$. Describe a method which can be used to get a good approximation for this value for a dataset.   (5 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQc1dZ7vpfqf"
      },
      "source": [
        "ANSWER: One method which can be used to get a good approximation of k is to use the 'Elbow Curve Method'. In this method the following steps are performed -\n",
        "\n",
        "1. Choose a range of k values, say from 1-10.\n",
        "2. For each of the k values, perform K-means clustering and calculate the average of the squared distances from the cluster centroids of the respective clusters for all the data points.\n",
        "3. For each value of k, plot the distance calculated. \n",
        "4. The plot obtained will have a curve that looks like an elbow, from where, the distance value suddenly falls flat for increasing values of k.\n",
        "5. The elbow point of k is chosen to be the optimal k for the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPjUFcjivMnA"
      },
      "source": [
        "**Question 4.2:** In this assignment, the initial centroids are given. The K-Means clustering output depends on the initial seeds. Therefore, different random seed selection would lead to different clustering outcomes. Describe a seed selection method that can lead to better K-Means clustering. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoeiB3imv4u8"
      },
      "source": [
        "ANSWER: There are many ways to initialize the centroids of the clusters in k-means clustering. One of the method that can lead to better k-means clustering is 'k-means++' or 'KMPP'. In this method, the first centre is chosen randomly from the given dataset. Then, the second centre is also chosen randomly from the dataset, but, the probability of selection of the datapoint is proportional to the euclidian distance of it to that of the 1st centre. Then, the third centre is again chosen randomly with the probability of selection proportional to the euclidian distance of the data point to the nearest of the previous two centres. Similarly, in the same way, subsequent centres can be calculated. This way, the k centres for the clusters can be initialized. "
      ]
    }
  ]
}