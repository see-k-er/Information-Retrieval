{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKC3O3QiIi4"
      },
      "source": [
        "#COMPSCI 546: Applied Information Retrieval ([website](https://groups.cs.umass.edu/zamani/compsci-546-applied-information-retrieval-spring-2022/))\n",
        "##Assignment 1: Information Retrieval Metrics (Total : 100 points)\n",
        "\n",
        "**Description**\n",
        "\n",
        "This is a coding assignment where you will write and execute code to evaluate ranked outputs generated by a retrieval model or a recommender system. Basic proficiency in Python is recommended.  \n",
        "\n",
        "**Instructions**\n",
        "\n",
        "* To start working on the assignment, you would first need to save the notebook to your local Google Drive. For this purpose, you can click on *Copy to Drive* button. You can alternatively click the *Share* button located at the top right corner and click on *Copy Link* under *Get Link* to get a link and copy this notebook to your Google Drive.  \n",
        "\n",
        "*   For questions with descriptive answers, please replace the text in the cell which states \"Enter your answer here!\" with your answer. If you are using mathematical notation in your answers, please define the variables.\n",
        "*   You should implement all the functions yourself and should not use a library or tool for computing the metrics.\n",
        "*   For coding questions, you can add code where it says \"enter code here\" and execute the cell to print the output.\n",
        "* To create the final pdf submission file, execute *Runtime->RunAll* from the menu to re-execute all the cells and then generate a PDF using *File->Print->Save as PDF*. Make sure that the generated PDF contains all the codes and printed outputs before submission. You are responsible for uploading the correct pdf with all the information required for grading.\n",
        "To create the final python submission file, click on File->Download .py.\n",
        "\n",
        "\n",
        "**Submission Details**\n",
        "\n",
        "* Due data: Feb. 10, 2022 at 11:59 PM (EST).\n",
        "* The final PDF and python file must be uploaded on Moodle.\n",
        "* After copying this notebook to your Google Drive, please paste a link to it below. Use the same process given above to generate a link. ***You will not recieve any credit if you don't paste the link!*** Make sure we can access the file.\n",
        "***LINK: ***\n",
        "\n",
        "**Academic Honesty**\n",
        "\n",
        "Please follow the guidelines under the *Collaboration and Help* section of the course website.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVqvXU6ijXi"
      },
      "source": [
        "# Download input files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMU5d1a-jCG-"
      },
      "source": [
        "Please execute the cell below to download the input files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM1igiBMcIB8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "download = drive.CreateFile({'id': '1myaSouVnJygjLlQI54_S0vRXLNYekEC8'})\n",
        "download.GetContentFile('HW01.zip')\n",
        "\n",
        "with zipfile.ZipFile('HW01.zip', 'r') as zip_file:\n",
        "    zip_file.extractall('./')\n",
        "os.remove('HW01.zip')\n",
        "# We will use hw1 as our working directory\n",
        "os.chdir('HW01')\n",
        "\n",
        "#Setting the input files\n",
        "qrel_file = \"antique-train-final.qrel\"\n",
        "rank_file = \"ranking_file\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWB3Hw6UKiPd"
      },
      "source": [
        "# 1: Initial Data Setup (10 Points) \n",
        "\n",
        "We use the files from the ANTIQUE dataset [https://arxiv.org/pdf/1905.08957.pdf] for this assignment. This is a passage retrieval dataset for non-factoid questions created by the Center for Intelligent Information Retrieval (CIIR) at UMass Amherst.\n",
        "\n",
        "The description of the input files provided for this assignment is given below. \n",
        "\n",
        "**1) Query Relevance (qrel) file**\n",
        "\n",
        "The qrel file contains the relevance judgements (ground truth) for the query passage combinations. The file consists of 4 columns with the information given below.\n",
        "\n",
        "*\\[queryid]  [topicid]  [passageid]  [relevancejudgment]*\n",
        "\n",
        "Entries in each row are space separated. The second column (topicid) can be ignored. \n",
        "\n",
        "Given below are a couple of rows of a sample qrel file.\n",
        "\n",
        "*2146313 U0 2146313_0 4*\n",
        "\n",
        "*2146313 Q0 2146313_23 2*\n",
        "\n",
        "The relevance judgements range from values 1-4. \n",
        "The description of the labels is given below:\n",
        "\n",
        "Label 1: Non-Relevant\n",
        "\n",
        "Label 2: Slightly Relevant\n",
        "\n",
        "Label 3 : Relevant\n",
        "\n",
        "Label 4: Highly Relevant\n",
        "\n",
        "**Note**: that for metrics with binary relevance assumptions, Labels 1 and 2 are considered non-relevant and Labels 3 and 4 are considered relevant.\n",
        "\n",
        "**Note**: if a query-document pair is not listed in the qrels file, we assume that the document is not relevant to the query.\n",
        "\n",
        "**2) Ranking file**\n",
        "\n",
        "The evaluation metric value has to be calculated for the input ranking file. The file was generated using a standard search engine by executing a ranking model, retrieving the top 100 passages for each of the train queries of the ANTIQUE dataset. The format of this file is given below. \n",
        "\n",
        "*\\[queryid]  [topicid]  [passageid]  [rank] [relevance_score]  [indri]*\n",
        "\n",
        "Similar to the qrel file, the entries in each row are space delimited.\n",
        "\n",
        "Given below are some sample examples of the ranking file contents.\n",
        "\n",
        "*3097310 Q0 2367043_3 1 -6.01785 indri*\n",
        "\n",
        "*3097310 Q0 3007432_0 2 -6.22531 indri*\n",
        "\n",
        "*3097310 Q0 674672_2 3 -6.28514 indri*\n",
        " \n",
        "**Note**: For this assignment, you would only need the information from Column 1(queryid) and Column 3(passageid). The passages corresponding to each query is ranked with respect to the relevance score (highest to lowest), therefore you would not need to use Column 4 (rank) explicitly. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoWda3QKIiPr"
      },
      "source": [
        "In order to make it easier to access this information in subsequent cells, please store them in appropriate data structures in the cell below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgJPt-BpKi0q",
        "outputId": "7a6af231-f850-4538-8f36-aed4da13cee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Num of queries in the qrel file  : 2426\n",
            "Total Num of queries in the rank file  : 2426\n",
            "Total number of relevant passages in the dataset : 19813\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "''' \n",
        "In this function, load the qrel file into qrel datastructure \n",
        "Return Variables: \n",
        "num_queries_1 - Number of unique queries in the qrel file\n",
        "num_rel - Number of total relevant passages in the dataset across all queries\n",
        "qrels - datastructure with the query passage relevance information\n",
        "'''\n",
        "def loadQrels(qrel_file):\n",
        "     #enter your code here\n",
        "     qid = []\n",
        "     pid =[]\n",
        "     tup = []\n",
        "     reljudg = []\n",
        "     with open(qrel_file) as f:   \n",
        "       for line in f:\n",
        "         temp = line.split(\" \")\n",
        "         qid.append(temp[0])\n",
        "         pid.append(temp[2])\n",
        "         reljudg.append(temp[3].strip('\\n'))\n",
        "         tup.append((temp[0], temp[2]))\n",
        "\n",
        "     qrels = pd.DataFrame({'key_val': tup, 'query_id':qid, 'passage_id':pid, 'relevance_judge':reljudg})\n",
        "     #replacing relevance values > 2 as 1 and < 2 as 0\n",
        "     qrels['normal_relevance_judge'] = qrels['relevance_judge'].apply(lambda x: '0' if (x == '1' or x == '2') else '1')    \n",
        "\n",
        "     num_queries_1 = len(pd.unique(qrels['query_id']))\n",
        "     num_rel = len(qrels[((qrels['normal_relevance_judge'] == \"1\"))])\n",
        "     \n",
        "\n",
        "     return num_queries_1, num_rel, qrels\n",
        "\n",
        "\n",
        "\n",
        "''' \n",
        "In this function, load the ranking files into rank_in datastructure \n",
        "Return Variables: \n",
        "num_queries_2 - Number of unique queries in the ranking file\n",
        "rank_in - datastructure with stored ranking information\n",
        "'''     \n",
        "def  loadRankfile(rank_file):\n",
        "      #enter your code here\n",
        "      qid = []\n",
        "      pid = []\n",
        "      rank = []\n",
        "      rank_in = {}\n",
        "      with open(rank_file) as f:   \n",
        "       for line in f:\n",
        "         temp = line.split(\" \")\n",
        "         qid.append(temp[0])\n",
        "         pid.append(temp[2])\n",
        "         rank.append(temp[3])\n",
        "\n",
        "      rank_in = pd.DataFrame({\"query_id\":qid, \"passage_id\":pid, \"rank\":rank})  \n",
        "\n",
        "      num_queries_2 = len(pd.unique(rank_in['query_id']))\n",
        "\n",
        "      return num_queries_2, rank_in\n",
        "      \n",
        "\n",
        "\n",
        "''' You can return single/multiple variables to store data if that makes it convenient \n",
        "for data processing. \n",
        "This has been given as an example. However, you would still need to correctly print the \n",
        "queries in both files and total relevant passages.'''\n",
        "num_queries_1, num_rel, qrels  = loadQrels(qrel_file)\n",
        "num_queries_2, rank_in = loadRankfile(rank_file)\n",
        "\n",
        "# print to ensure the file has been read correctly\n",
        "print ('Total Num of queries in the qrel file  : {0}'.format(num_queries_1))\n",
        "print ('Total Num of queries in the rank file  : {0}'.format(num_queries_2))\n",
        "print ('Total number of relevant passages in the dataset : {0}'.format(num_rel))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qrels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbT3I8bYhJLP",
        "outputId": "2ea0815a-b13c-4e2b-b806-05555fc6870d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    key_val query_id  ... relevance_judge normal_relevance_judge\n",
            "0      (2531329, 2531329_0)  2531329  ...               4                      1\n",
            "1      (2531329, 2531329_5)  2531329  ...               4                      1\n",
            "2      (2531329, 2531329_4)  2531329  ...               3                      1\n",
            "3      (2531329, 2531329_7)  2531329  ...               3                      1\n",
            "4      (2531329, 2531329_6)  2531329  ...               3                      1\n",
            "...                     ...      ...  ...             ...                    ...\n",
            "27417    (884731, 884731_0)   884731  ...               4                      1\n",
            "27418    (884731, 884731_4)   884731  ...               4                      1\n",
            "27419    (884731, 884731_2)   884731  ...               4                      1\n",
            "27420    (884731, 884731_3)   884731  ...               4                      1\n",
            "27421    (884731, 884731_1)   884731  ...               3                      1\n",
            "\n",
            "[27422 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh5fnRSB4Q-J",
        "outputId": "c417f2e8-eac0-4dec-a855-c5d06517f1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       query_id  passage_id rank\n",
            "0       3097310   2367043_3    1\n",
            "1       3097310   3007432_0    2\n",
            "2       3097310    674672_2    3\n",
            "3       3097310   2311700_0    4\n",
            "4       3097310   2606613_4    5\n",
            "...         ...         ...  ...\n",
            "242595  4086230  2633809_23   96\n",
            "242596  4086230   3391788_0   97\n",
            "242597  4086230   597728_11   98\n",
            "242598  4086230   4356539_1   99\n",
            "242599  4086230  2261458_20  100\n",
            "\n",
            "[242600 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "print(rank_in)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qrel_dict = qrels.set_index('key_val').to_dict()['normal_relevance_judge'] #creating hashmap with (qid,pid) as key and relevance_judge as value"
      ],
      "metadata": {
        "id": "YvNSJrhvkCkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmtxE4uPrN-6"
      },
      "source": [
        "\n",
        "\n",
        "# 2 : Precision (15 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUa5rRXruIG"
      },
      "source": [
        "Question 2.1 (5 points)\n",
        "\n",
        "Give the definition of Precision corresponding to the top *k* retrieved results for *n* queries (P@k). Please note that you have to use averaging to aggregate the results from all queries.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFvJUJRur9k6"
      },
      "source": [
        "***Answer*** \n",
        "Precision at k is the number of relevant results among the top k retrieved documents. Mathematically, precision is a ratio of the number of total relevant retrieved documents within the top k retrieved results (ranks) to the total number of retrieved documents within the rank for a given query. When we need to calculate precision across all queries, we take the average precision of them across n queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOE4d9MqsM2M"
      },
      "source": [
        "\n",
        "\n",
        "Question 2.2 (10 points) \n",
        "\n",
        "In the cell below, please enter the code to print the P@k where k={5,10} for the input ranking file.  As mentioned above, please note that the final value is the average of metric values across all queries. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "In this function, calculate Precision@k, given the input ranking information (rank_in)  \n",
        "and the query passage relevance information (qrels).\n",
        "Return Value: \n",
        "precision - Precision@k\n",
        "'''\n",
        "\n",
        "import time\n",
        "def precCal(qid, k, qrels, rank_in):\n",
        "  data = rank_in.loc[(rank_in['query_id'] == qid)].head(k).reset_index()\n",
        "  ctr = 0\n",
        "  for index, row in data.iterrows():\n",
        "    if ((row['query_id'], row['passage_id'])) in qrel_dict:\n",
        "      if qrel_dict[(row['query_id'], row['passage_id'])] == '1':\n",
        "        ctr += 1\n",
        "      else: \n",
        "        ctr += 0 \n",
        "    else: \n",
        "        continue\n",
        "  #precision value for qid\n",
        "  prec = ctr/k\n",
        "    \n",
        "  return prec\n",
        "\n",
        "\n",
        "def calcPrecision(k, qrels, rank_in):\n",
        "    #unique queries\n",
        "    que = pd.unique(rank_in['query_id'])\n",
        "    qdf = pd.DataFrame(que, columns=['qid'])\n",
        "    qdf['precision'] = qdf['qid'].apply(lambda row: precCal(row, k, qrels, rank_in))\n",
        "    #final aggr precision value\n",
        "    precision = qdf['precision'].sum()/len(que)\n",
        "    return precision\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print ('Precision at top 5 : {0}'.format(calcPrecision(5, qrels, rank_in)))\n",
        "print ('Precision at top 10 : {0}'.format(calcPrecision(10, qrels, rank_in)))\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DlfRwVSCjUw",
        "outputId": "5d2a6040-c301-44b3-a94e-621e7c73e072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision at top 5 : 0.15539983511953834\n",
            "Precision at top 10 : 0.1072959604286892\n",
            "--- 107.90563535690308 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j55h8jAhtkcP"
      },
      "source": [
        "# 3 : Recall (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR_VxrTmtLn"
      },
      "source": [
        "Question 3.1 (5 points) \n",
        "\n",
        "Give the definition of Recall corresponding to the top *k* retrieved results for *n* queries (R@k). Please note that you have to use averaging to aggregate the results from all queries. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X1ufq8cmyzo"
      },
      "source": [
        "***Answer*** \n",
        "Recall at k is the ratio of the total number of relevant retrieved documents from the top k ranks, to the total number of relevant documents for a given query. Then, the average recall value is found across n queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQi0Kg4Vm3pn"
      },
      "source": [
        "Question 3.2 (10 points) \n",
        "\n",
        "In the cell below, please enter the code to print Recall (R@k) where k={5,10} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "In this function, calculate Recall@k, given the input ranking information (rank_in)  \n",
        "and the query passage relevance information (qrels).\n",
        "Return Value: \n",
        "recall - Recall@k\n",
        "'''\n",
        "\n",
        "def recCal(qid, k, qrels, rank_in):\n",
        "  recos = len(qrels[(qrels['query_id'] == qid) & ((qrels['normal_relevance_judge'] == \"1\"))])\n",
        "  data = rank_in.loc[(rank_in['query_id'] == qid)].head(k).reset_index()\n",
        "  ctr = 0\n",
        "  for index, row in data.iterrows():\n",
        "    if ((row['query_id'], row['passage_id'])) in qrel_dict:\n",
        "      if qrel_dict[(row['query_id'], row['passage_id'])] == '1':\n",
        "        ctr += 1\n",
        "      else: \n",
        "        ctr += 0 \n",
        "    else: \n",
        "        continue\n",
        "  #recall value for qid\n",
        "  rec = ctr/recos   \n",
        "  return rec\n",
        "\n",
        "\n",
        "def calcRecall(k, qrels, rank_in):\n",
        "    #unique queries\n",
        "    que = pd.unique(rank_in['query_id'])\n",
        "    qdf = pd.DataFrame(que, columns=['qid'])\n",
        "    qdf['recall'] = qdf['qid'].apply(lambda row: recCal(row, k, qrels, rank_in))\n",
        "    #final aggr recall value\n",
        "    recall = qdf['recall'].sum()/len(que)\n",
        "    return recall\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print ('Recall at top 5 : {0}'.format(calcRecall(5, qrels, rank_in)))\n",
        "print ('Recall at top 10 : {0}'.format(calcRecall(10, qrels, rank_in)))\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbLsdasfI4ic",
        "outputId": "55bffef3-db6d-438f-8936-a475c036f4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall at top 5 : 0.1293441649380108\n",
            "Recall at top 10 : 0.16743513404735452\n",
            "--- 131.22059774398804 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-QYucscmdlK"
      },
      "source": [
        "# 4 : F1 Measure (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-dG5UL-nem5"
      },
      "source": [
        "Question 4.1 (5 points) \n",
        "\n",
        "Give the definition of F1 measure corresponding to the top *k* retrieved results for *n* queries (F1@k). Please note that you have to use averaging to aggregate the results from all queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Ptg39nnlNt"
      },
      "source": [
        "***Answer*** \n",
        "F1 measure is basically the harmonic mean of precision and recall. It is the ratio of twice the value of precision multiplied with recall for top k ranks to the sum of precision and recall for top k ranks, for a given query. The average is calculated by taking the sum of all the F1 scores across all n queries and dividing it by the total number of unique queries (n). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5q6oMPYnnyB"
      },
      "source": [
        "Question 4.2 (10 points) \n",
        "\n",
        "In the cell below, please enter the code to print the F1@k where k={5,10} for the input ranking file.  Please note that you have to calculate F1 score for each query and compute the final score by averaging the metric values across all queries. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "In this function, calculate F1@k, given the input ranking information (rank_in)  \n",
        "and the query passage relevance information (qrels).\n",
        "Return Value: \n",
        "f1 - F1@k\n",
        "''' \n",
        "\n",
        "def f1Cal(qid, k, qrels, rank_in):\n",
        "  recos = len(qrels[(qrels['query_id'] == qid) & ((qrels['normal_relevance_judge'] == \"1\"))])\n",
        "  data = rank_in.loc[(rank_in['query_id'] == qid)].head(k).reset_index()\n",
        "  ctr = 0\n",
        "  for index, row in data.iterrows():\n",
        "    if ((row['query_id'], row['passage_id'])) in qrel_dict:\n",
        "      if qrel_dict[(row['query_id'], row['passage_id'])] == '1':\n",
        "        ctr += 1\n",
        "      else: \n",
        "        ctr += 0 \n",
        "    else: \n",
        "        continue\n",
        "\n",
        "  #precision value for qid\n",
        "  precision = ctr/k\n",
        "\n",
        "  #recall value for qid\n",
        "  recall = ctr/recos   \n",
        "\n",
        "  if precision + recall != 0:    \n",
        "    return f1score(precision, recall)\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def f1score(precision, recall):\n",
        "  return ((2*precision*recall)/(precision + recall))\n",
        "\n",
        "def calcFScore(k, qrels, rank_in):\n",
        "    #unique queries\n",
        "    que = pd.unique(rank_in['query_id'])\n",
        "    qdf = pd.DataFrame(que, columns=['qid'])\n",
        "    qdf['f1'] = qdf['qid'].apply(lambda row: f1Cal(row, k, qrels, rank_in))\n",
        "    #final aggr f1 value\n",
        "    f1 = qdf['f1'].sum()/len(que)\n",
        "    return f1\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print ('F1 score at top 5 : {0}'.format(calcFScore(5, qrels, rank_in)))\n",
        "print ('F1 score at top 10 : {0}'.format(calcFScore(10, qrels, rank_in))) \n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZwDYbmKLFLh",
        "outputId": "4c3bf5d9-de5c-4404-bd5c-eade61cdbbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score at top 5 : 0.12587171180786297\n",
            "F1 score at top 10 : 0.11669190136577237\n",
            "--- 120.64473867416382 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inXXPsyinO4A"
      },
      "source": [
        "# 5 : Mean Reciprocal Rank (MRR) (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJGcfaWQKazF"
      },
      "source": [
        "Question 5.1 (5 points)\n",
        "\n",
        "Give the definition of MRR@k corresponding to the top *k* retrieved results for *n* queries (MRR@k). Please note that you have to use averaging to aggregate the results from all queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juSenU8JLCl6"
      },
      "source": [
        "***Answer*** \n",
        "Reciprocal Rank in Information Retrieval is the inverse of the rank of the first relevant retrieved documents within the top k retrieved results for a given query. If no relevant document is retrieved, then its value is taken as 0. The Mean Reciprocal Rank is the Reciprocal Rank averaged across all n queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz9tZETfLI6c"
      },
      "source": [
        "Question 5.2 (10 points)\n",
        "\n",
        "In the cell below, please enter the code to print the MRR@k where k={5,10} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "In this function, calculate MRR@k, given the input ranking information (rank_in)  \n",
        "and the query passage relevance information (qrels).\n",
        "Return Value: \n",
        "mrr - MRR@k\n",
        "''' \n",
        "\n",
        "def mrrCal(qid, k, qrels, rank_in):\n",
        "  data = rank_in.loc[(rank_in['query_id'] == qid)].head(k).reset_index()\n",
        "  for index, row in data.iterrows():\n",
        "    if ((row['query_id'], row['passage_id'])) in qrel_dict:\n",
        "      if qrel_dict[(row['query_id'], row['passage_id'])] == '1':\n",
        "        rec_rank = 1/(index+1) #here index starts from 0, hence adding 1 for normal ranking\n",
        "        return rec_rank\n",
        "  return 0\n",
        "\n",
        "def calcMRR(k, qrels, rank_in):\n",
        "  #enter your code here\n",
        "  #unique queries\n",
        "  que = pd.unique(rank_in['query_id'])\n",
        "  qdf = pd.DataFrame(que, columns=['qid'])\n",
        "  qdf['mrr'] = qdf['qid'].apply(lambda row: mrrCal(row, k, qrels, rank_in))\n",
        "  #final aggr mrr value\n",
        "  mrr = qdf['mrr'].sum()/len(que)\n",
        "  return mrr\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print ('MRR at top 5 : {0}'.format(calcMRR(5, qrels, rank_in)))\n",
        "print ('MRR at top 10 : {0}'.format(calcMRR(10, qrels, rank_in)))   \n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NR3gXLnVHJ7",
        "outputId": "69d3dc3d-8d79-44b0-d788-cfa886c3f5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR at top 5 : 0.3375996152789228\n",
            "MRR at top 10 : 0.34748982582865523\n",
            "--- 97.61747598648071 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HypgYsTAPcON"
      },
      "source": [
        "# 6 : Mean Average Precision (MAP) (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy69CA4wPinF"
      },
      "source": [
        "Question 6.1 (5 points)\n",
        "\n",
        "Give the definition of MAP@k corresponding to the top *k* retrieved results for *n* queries (MAP@k). Please note that you have to use averaging to aggregate the results from all queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y3FDKGiPopO"
      },
      "source": [
        "***Answer*** \n",
        "The average precision is the most common metric in TREC. Here, the precision is computed whenever a relevant document is found and then it is averaged. \n",
        "Mathematically average precision for a given query is given by -\n",
        "\n",
        "$AP = \\frac{1}{|R|} \\sum_{i=1}^k Precision_i Relevance_i$ \\\\\n",
        "\n",
        "where, |R| is the total number of relevant documents \\\\\n",
        "k is the length of the rank list \\\\\n",
        "$Precision_i$ is the precision of the top i documents \\\\\n",
        "$Relevance_i$ is the relevance of the ith document \\\\\n",
        "\n",
        "Then, the mean average precision is calculated by taking the average of average precision across n queries. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brN7ozRePtd-"
      },
      "source": [
        "Question 6.2 (10 points)\n",
        "\n",
        "In the cell below, please enter the code to print the MAP@k where k={50, 100} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgj5MTwyP6iC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a5ce1d-1fca-429d-99ba-000cc0361241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP at top 50 : 0.12129151696866383\n",
            "MAP at top 100 : 0.12326331463633428\n",
            "--- 7993.947522163391 seconds ---\n"
          ]
        }
      ],
      "source": [
        "''' \n",
        "In this function, calculate MAP@k, given the input ranking information (rank_in)  \n",
        "and the query passage relevance information (qrels).\n",
        "Return Value: \n",
        "map - MAP@k\n",
        "'''\n",
        "\n",
        "def mapCal(qid, k, qrels, rank_in):\n",
        "  recos = len(qrels[(qrels['query_id'] == qid) & ((qrels['normal_relevance_judge'] == \"1\"))])\n",
        "  sump = 0\n",
        "  for j in range(1, k):\n",
        "    relevance_val = 0\n",
        "    data = rank_in.loc[(rank_in['query_id'] == qid)].head(j).reset_index()\n",
        "    ctr = 0\n",
        "    temp = 0 \n",
        "    for index, row in data.iterrows():\n",
        "      temp += 1\n",
        "      if ((row['query_id'], row['passage_id'])) in qrel_dict:\n",
        "        if qrel_dict[(row['query_id'], row['passage_id'])] == '1':\n",
        "          ctr += 1\n",
        "          if temp == j:\n",
        "            relevance_val = 1\n",
        "        else: \n",
        "          ctr += 0 \n",
        "\n",
        "    #precision value for qid and relevance at j\n",
        "    precision = ctr/j\n",
        "    prod_p_r = precision*relevance_val\n",
        "\n",
        "    sump += prod_p_r\n",
        "\n",
        "  #averaging over all the relevant docs for a single qid\n",
        "  ap = sump/recos\n",
        "  return ap\n",
        "\n",
        "\n",
        "def calcMAP(k, qrels, rank_in):\n",
        "  #enter your code here\n",
        "  #unique queries\n",
        "  que = pd.unique(rank_in['query_id'])\n",
        "  qdf = pd.DataFrame(que, columns=['qid'])\n",
        "  qdf['map'] = qdf['qid'].apply(lambda row: mapCal(row, k, qrels, rank_in))\n",
        "  #final aggr map value\n",
        "  map = qdf['map'].sum()/len(que)\n",
        "  return map\n",
        "   \n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print ('MAP at top 50 : {0}'.format(calcMAP(50, qrels, rank_in)))\n",
        "print ('MAP at top 100 : {0}'.format(calcMAP(100, qrels, rank_in)))   \n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFGe-z67KRfY"
      },
      "source": [
        "# 7 : Normalized Discounted Cumulative Gain (NDCG) (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPBQdRW4KWur"
      },
      "source": [
        "Question 7.1 (5 points)\n",
        "\n",
        "Give the definition of NDCG@k corresponding to the top *k* retrieved results for *n* queries (NDCG@k). Use the definition discussed in the lectures. Note that this metric considers graded relevance judgments and you should not binarize the labels. To assign zero gain to non-relevant documents, decrease all relevance labels in the ANTIQUE qrels by 1 point i.e. map relevance judgements 1-4 to 0-3. Please note that you have to use averaging to aggregate the results from all queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W1VjH8CKa4V"
      },
      "source": [
        "***Answer*** \n",
        "Normalized DCG for a query is the ratio of Discounted Cumulative Gain (DCG) to the Ideal DCG. \n",
        "Discounted Cumulative gain at rank k is mathematically given as - \\\\\n",
        "\n",
        "$DCG = \\frac{r_1}{\\log_{2}2} + \\frac{r_1}{\\log_{2}3} + \\frac{r_1}{\\log_{2}4} +  ...  + \\frac{r_k}{\\log_{2}(k+1)}$ \\\\\n",
        "\n",
        "Ideal DCG at rank k for a query is the top k results from the full ideal ranking (when queries with relevance ranks are sorted in descending order).\n",
        "\n",
        "Normalized Discounted Cumulative Gain usually has values between [0,1].\n",
        "\n",
        "The average NDCG value across n queries is calculated by taking the sum of ndcg across all n queries and dividing it by n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LY6e8NDKdWl"
      },
      "source": [
        "Question 7.2 (10 points)\n",
        "\n",
        "In the cell below, please enter the code to print the NDCG@k where k={5, 10} for the input ranking file. As mentioned above, please note that the final value is the average of metric values across all queries. \n",
        "\n",
        "Use log base 2 for the calculations. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling down the relevance labels\n",
        "qrels['relevance_judge'] = qrels['relevance_judge'].apply(lambda x: str(int(x) - 1))\n",
        "rel_dict_dcg = qrels.set_index('key_val').to_dict()['relevance_judge']"
      ],
      "metadata": {
        "id": "bwaf8juD3cu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' \n",
        "In this function, calculate NDCG@k, given the input ranking information (rank_in)  \n",
        "and the query passage relevance information (qrels).\n",
        "Return Value: \n",
        "ndcg - NDCG@k\n",
        "'''\n",
        "\n",
        "import math\n",
        "def dcgCal(qid, k, qrels, rank_in):\n",
        "  recos = qrels[(qrels['query_id'] == qid) & ((qrels['relevance_judge'] == \"2\") | (qrels['relevance_judge'] == \"3\"))].sort_values('relevance_judge', ascending=False).head(k).reset_index()\n",
        "  data = rank_in.loc[(rank_in['query_id'] == qid)].head(k).reset_index()\n",
        "  dcg = 0\n",
        "  idcg = 0\n",
        "  denom_start = 2\n",
        "  dnm_start = 2\n",
        "  ctr = 0\n",
        "  for index, row in data.iterrows():\n",
        "    if ((row['query_id'], row['passage_id'])) in qrel_dict:\n",
        "      r_at_index = int(rel_dict_dcg[(row['query_id'], row['passage_id'])])\n",
        "      dcg += (r_at_index/math.log(denom_start, 2))\n",
        "    else:\n",
        "      denom_start += 1\n",
        "    denom_start += 1\n",
        "\n",
        "    #to find ideal dcg\n",
        "    rval = int(recos['relevance_judge'][ctr])\n",
        "    idcg += rval/math.log(dnm_start, 2)\n",
        "    dnm_start += 1\n",
        "\n",
        "  ndcg = dcg/idcg\n",
        "\n",
        "  return ndcg\n",
        "\n",
        "\n",
        "def calcNDCG(k, qrels, rank_in):\n",
        "    #unique queries\n",
        "    que = pd.unique(rank_in['query_id'])\n",
        "    qdf = pd.DataFrame(que, columns=['qid'])\n",
        "    qdf['ndcg'] = qdf['qid'].apply(lambda row: dcgCal(row, k, qrels, rank_in))\n",
        "    #final aggr ndcg value\n",
        "\n",
        "    ndcg = qdf['ndcg'].sum()/len(que)\n",
        "    return ndcg\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print ('NDCG at top 5 : {0}'.format(calcNDCG(5, qrels, rank_in)))\n",
        "print ('NDCG at top 10 : {0}'.format(calcNDCG(10, qrels, rank_in)))  \n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gv6eSZgn3Xa",
        "outputId": "6fb6897c-48d3-488f-aad0-e30e070620c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG at top 5 : 0.1646480959858136\n",
            "NDCG at top 10 : 0.12391912424164493\n",
            "--- 138.52916717529297 seconds ---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS546 HW1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
