{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS546_HW5_S22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKC3O3QiIi4"
      },
      "source": [
        "#COMPSCI 546: Applied Information Retrieval - Spring 2022 ([website](https://groups.cs.umass.edu/zamani/compsci-546-applied-information-retrieval-spring-2022/))\n",
        "##Assignment 5: TextRank (Total : 100 points)\n",
        "\n",
        "**Description**\n",
        "\n",
        "This is a coding assignment where you will implement the TextRank Algorithm to extract keywords for a document. Basic proficiency in Python is recommended.  \n",
        "\n",
        "**Instructions**\n",
        "\n",
        "* To start working on the assignment, you would first need to save the notebook to your local Google Drive. For this purpose, you can click on *Copy to Drive* button. You can alternatively click the *Share* button located at the top right corner and click on *Copy Link* under *Get Link* to get a link and copy this notebook to your Google Drive.  \n",
        "\n",
        "*   For questions with descriptive answers, please replace the text in the cell which states \"Enter your answer here!\" with your answer. If you are using mathematical notation in your answers, please define the variables.\n",
        "*   You should implement all the functions yourself and should not use a library or tool for the computation.\n",
        "*   For coding questions, you can add code where it says \"enter code here\" and execute the cell to print the output.\n",
        "* To create the final pdf submission file, execute *Runtime->RunAll* from the menu to re-execute all the cells and then generate a PDF using *File->Print->Save as PDF*. Make sure that the generated PDF contains all the codes and printed outputs before submission.\n",
        "To create the final python submission file, click on File->Download .py.\n",
        "\n",
        "\n",
        "**Submission Details**\n",
        "\n",
        "* Due data: Mar. 31, 2021 at 11:59 PM (EDT).\n",
        "* The final PDF and python file must be uploaded on Moodle.\n",
        "* After copying this notebook to your Google Drive, please paste a link to it below. Use the same process given above to generate a link. ***You will not recieve any credit if you don't paste the link!*** Make sure we can access the file.\n",
        "***LINK: *https://colab.research.google.com/drive/1geo3YAKRQHTJZ4y-IEnHWF-sxYq14N2j?usp=sharing***\n",
        "\n",
        "**Academic Honesty**\n",
        "\n",
        "Please follow the guidelines under the *Collaboration and Help* section of the course website.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVqvXU6ijXi"
      },
      "source": [
        "# Download input files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMU5d1a-jCG-"
      },
      "source": [
        "Please execute the cell below to download the input files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM1igiBMcIB8"
      },
      "source": [
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "download = drive.CreateFile({'id': '1nx6a70YBfOBT357ujg0MDI4TDj8yWheT'})\n",
        "download.GetContentFile('HW05.zip')\n",
        "\n",
        "with zipfile.ZipFile('HW05.zip', 'r') as zip_file:\n",
        "    zip_file.extractall('./')\n",
        "os.remove('HW05.zip')\n",
        "# We will use hw05 as our working directory\n",
        "os.chdir('HW05')\n",
        "\n",
        "#Setting the input file\n",
        "doc = \"covid_nyt.tok.clean_nostop\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L84x5CFj9Z-L"
      },
      "source": [
        "# 1 : Initial Data Setup (20 points)\n",
        "\n",
        "The input file consists of a single long document which is a news article about Covid. This file has been pre-processed to remove punctuation, non-alphanumeric characters and stopwords as well as tokenized such that the terms are space separated. Please note that this file contains a single long line. \n",
        "\n",
        "In the TextRank algorithm, we create a graph corresponding to the document, where each node is a term and the edge between the terms implies that they co-occur within a window size $w$. \n",
        "\n",
        "In the cell below, you have to implement the following:\n",
        "\n",
        "1) Generate vocabulary from the text. This is the unique set of terms in the text. Each term must be mapped to a unique integer [0,vocab_size-1].\n",
        "\n",
        "2) Generate all term-pairs which co-occurs within a window. For this implementation, we will set the window size $w=2$. Please note that the window is overlapping. For example: for the text \"the sun rises\", the term pairs are [\"the\",\"sun\"] and [\"sun\",\"rises\"].\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RifFW_adR48S",
        "outputId": "eee4d9a7-d7c3-438c-f9d7-77953ac16075"
      },
      "source": [
        "''' \n",
        "In this function, iterate through the input file and store the vocabulary terms \n",
        "and term pairs which co-occurs within a window of 2.  \n",
        "Return Variables: \n",
        "vocab - dict which consists of term as key and the value is an integer [0,vocab_size-1]\n",
        "termPairs - All term pairs which co-occur within a window size 2. \n",
        "            This would be the entire set, without removing the repeating term pairs.\n",
        "idVocab - list of words where the index corresponds to the vocab dict value for that word. \n",
        "          This is used to map ids to words.                \n",
        "'''\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def genInit(doc, size):\n",
        "  vocab = defaultdict(int)\n",
        "  idVocab = []\n",
        "  termPairs = [] \n",
        "\n",
        "  #vocab\n",
        "  file = open(doc, \"r\")\n",
        "  for v in file:\n",
        "    temp = v.split()\n",
        "    uniq = set(temp)\n",
        "    count = 0\n",
        "    for i in uniq:\n",
        "        vocab[i] = count\n",
        "        count += 1\n",
        "  file.close()\n",
        "\n",
        "  #termPairs\n",
        "  lp = 0\n",
        "  rp = size\n",
        "  while lp < len(temp) - 1 and rp <= len(temp):\n",
        "    termPairs.append(temp[lp:rp])\n",
        "    lp += 1\n",
        "    rp += 1\n",
        "\n",
        "  #idVocab\n",
        "  idVocab = [0] * len(vocab)\n",
        "  for k,v in vocab.items():\n",
        "    idVocab[v] = k\n",
        "\n",
        "  return vocab, termPairs, idVocab\n",
        "\n",
        "size = 2\n",
        "vocab, termPairs, idVocab = genInit(doc, size)\n",
        "\n",
        "print('Total number of unique terms in the collection :{0}'.format(len(vocab)))\n",
        "print('Total number of term pairs :{0}'.format(len(termPairs)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique terms in the collection :1640\n",
            "Total number of term pairs :3464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V1g6gnbGrtR"
      },
      "source": [
        "# 2 : Transition Matrix Creation (30 points)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Let the vocab_size be $n$. The TextRank algorithm creates a weighted graph based on term co-occurrences. However, in this assignment, for the sake of simplicity, we will assume that the edges are unweighted and undirected. \n",
        "\n",
        "In the cell below, implement the following steps:\n",
        "\n",
        "1) Create a transition matrix $M = n\\times n$ where the value of each cell $M_{i,j}=1$, if the $i$-th and the $j$-th words co-occur within a window and $0$ otherwise. This can be implemented by iterating through the term pairs list and getting the integer value mapping for each term from the vocab dict and using these to index into the matrix. Note that since this is an undirected graph, $M_{ij} = M_{ji}$ for all $i$s and $j$s. \n",
        "\n",
        "2) Normalize the matrix such that, $\\forall i: \\sum_j M_{ij}=1$ i.e., divide the row elements by the sum of the row elements. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxK_nQ0mMTe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5067909e-047a-477d-ca50-436cf4492a16"
      },
      "source": [
        "''' \n",
        "In this function, create the transition matrix for the input document.  \n",
        "Return Variables: \n",
        "init_matrix - transition matrix containing the 0 or 1 element values depending on co-occurence. \n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "def createMatrix(vocab, termPairs):\n",
        "    size = len(vocab)\n",
        "    init_matrix = np.zeros((size, size))\n",
        "    for terms in termPairs:\n",
        "      i = vocab[terms[0]]\n",
        "      j = vocab[terms[1]]\n",
        "      init_matrix[i][j] = 1\n",
        "      init_matrix[j][i] = 1\n",
        "    return init_matrix\n",
        "\n",
        "\n",
        "''' \n",
        "In this function, normalize the transition matrix such that sum of the elements of a row is 1.  \n",
        "Return Variables: \n",
        "norm_matrix - normalized transition matrix\n",
        "'''\n",
        "def normalizeMatric(init_matrix):\n",
        "    numRows = init_matrix.shape[0]\n",
        "\n",
        "    for i in range(numRows):\n",
        "      rowTot = init_matrix[i].sum()\n",
        "      init_matrix[i] = init_matrix[i]/rowTot\n",
        "    norm_matrix = init_matrix\n",
        "  \n",
        "    return norm_matrix\n",
        "\n",
        "\n",
        "\n",
        "init_matrix = createMatrix(vocab, termPairs)\n",
        "norm_matrix = normalizeMatric(init_matrix)\n",
        "\n",
        "print('Shape of the transition matrix :{0}'.format(np.shape(norm_matrix)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the transition matrix :(1640, 1640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4W-p26pOwON"
      },
      "source": [
        "# 3 : TextRank -- PageRank Algorithm over the Constructed Graph of Terms (40 points)\n",
        "\n",
        "In the cell below, implement the PageRank Algorithm on the created graph by executing the following.\n",
        "\n",
        "$$p^{t+1} = (\\frac{\\alpha}{n} + (1-\\alpha) M)^T p^t $$\n",
        "\n",
        "$t$ is the iteration number starting from $0$.\n",
        "\n",
        "$p^t$ is a  $n \\times 1$ matrix where each row corresponds to a word.\n",
        "\n",
        "$p^0$ is initialized randomly. In other words, $p^0$ is a random $n \\times 1$ matrix with a length of 1. You can generate a random $n \\times 1$ matrix and divide all elements by their sum.\n",
        "\n",
        "$\\alpha$ is the random jump probability. Set $\\alpha=0.15$.\n",
        "\n",
        "The superscript $T$ denotes *transpose* of the given matrix.\n",
        "\n",
        "Execute this for 50 iterations to ensure convergence for this particular example. \n",
        "\n",
        "After the final iteration, display the top 10 terms with highest PageRank scores in $p^{50}$. These terms are supposed to be the keywords of the document.\n",
        "\n",
        "**Hint:** as a sanity check to make sure that the implementation is correct, make sure that the sum of all elements in every $p^t$ is equal to 1 (there may be an epsilon difference due to floating point calculations, so you can expect $1 \\pm \\epsilon$ where $\\epsilon < 10^{-10}$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GTmJkJ8QBW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ed42ed-d497-40ad-f8b8-456a4de9271e"
      },
      "source": [
        "\n",
        "''' \n",
        "In this function, implement PageRank Algorithm.   \n",
        "Return Variables: \n",
        "wordWeights - Return top 10 terms with highest weights.\n",
        "'''\n",
        "\n",
        "def pageRank(norm_matrix, vocab, idVocab):\n",
        "   alpha = 0.15\n",
        "   numRows = norm_matrix.shape[0]\n",
        "   vec = np.random.randint(0, 2, size=numRows)\n",
        "   totvec = vec.sum()\n",
        "   pt = vec/totvec\n",
        "   pt_new = np.reshape(pt, (numRows, 1))\n",
        "\n",
        "   for t in range(0, 50):\n",
        "     pt1 = np.transpose((alpha/numRows) + ((1-alpha)*norm_matrix)).dot(pt_new)\n",
        "     pt_new = pt1\n",
        "     \n",
        "   wordWeights = pt_new\n",
        "   return wordWeights\n",
        "\n",
        "\n",
        "wordWeights = pageRank(norm_matrix, vocab, idVocab)\n",
        "''' \n",
        "Hint: You don't have to display the word weight values. This is only for debugging.\n",
        "The weight of the top keyword is in the interval [0.0090,0.0095].\n",
        "The weight of the 10th top keyword is in the interval [0.0040,0.0045].\n",
        "'''\n",
        "print('Top 10 keywords :{0}'.format(np.array(idVocab)[np.argsort(wordWeights,axis=0)[::-1][:10]]))\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 keywords :[['said']\n",
            " ['people']\n",
            " ['vaccine']\n",
            " ['schools']\n",
            " ['pandemic']\n",
            " ['health']\n",
            " ['coronavirus']\n",
            " ['vaccines']\n",
            " ['year']\n",
            " ['also']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLZhgpnQpHHV"
      },
      "source": [
        "# 4: Explain PageRank (10 points)\n",
        "\n",
        "**Question 4.1.** As explained in the lecture, PageRank also considers indirect links in the graph. Explain Why. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQc1dZ7vpfqf"
      },
      "source": [
        "**ANSWER**\n",
        "\n",
        "PageRank basically is an inlink counting approach which follows the random walk user model. In this model, the random walk is started at random page and then on a number of iterations, it follows the along one of the link with equal probability. To escape from dead ends, and to reach all the nodes, a random jump to any node is also done with a certain probabilty - alpha. Thus, we can say that, PageRank doesn't only consider direct links, it also considers indirect links in order to randomly jump to an appropriate node with equal probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tto4qS-Bpi38"
      },
      "source": [
        "**Question 4.2.** Provide an intuitive explanation of why terms with high PageRank scores in the constructed graph in this assignment would be good keywords for the associated document. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uGgwhhIbOF6"
      },
      "source": [
        "**ANSWER**\n",
        "\n",
        "Terms which have high PageRank scores in the constructed graph implies that there are many links to that particular term in the document or there are other terms with high PageRank scores which are linked to that particular term in the document. This means that those terms with high PageRank scores would be good keywords for that document as those terms are very frequently visited in the terms graph that was constructed and they are also linked with a lot of the other terms within the document. This gives a higher chance for the document to be visited when referred with the terms with high PageRank scores."
      ]
    }
  ]
}