{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KKC3O3QiIi4"
      },
      "source": [
        "#COMPSCI 546: Applied Information Retrieval\n",
        "##Assignment 3: Retrieval Models (Total : 100 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVqvXU6ijXi"
      },
      "source": [
        "# Download input files and code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMU5d1a-jCG-"
      },
      "source": [
        "Please execute the cell below to download the input files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM1igiBMcIB8"
      },
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "import zipfile\n",
        "\n",
        "download = drive.CreateFile({'id': '1obnYvxGG8-x025j2U8aVYf4yBc5mFQsw'})\n",
        "download.GetContentFile('HW03.zip')\n",
        "\n",
        "with zipfile.ZipFile('HW03.zip', 'r') as zip_file:\n",
        "    zip_file.extractall('./')\n",
        "os.remove('HW03.zip')\n",
        "# We will use hw1 as our working directory\n",
        "os.chdir('HW03')\n",
        "\n",
        "#Setting the input files \n",
        "queries_file = \"queries_tok_clean_kstem\"\n",
        "col = \"antique-collection.tok.clean_kstem\"\n",
        "qrel_file = \"test.qrel\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9GqDu_ilscP"
      },
      "source": [
        "# 1 : Initial Data Setup (10 points)\n",
        "\n",
        "We use files from the ANTIQUE  [https://arxiv.org/pdf/1905.08957.pdf] dataset for this assignment. As described in the previous assignments, this is a passage retrieval dataset. \n",
        "\n",
        "The description of the input files provided for this assignment is given below.\n",
        "\n",
        "**Query File**\n",
        "\n",
        "We randomly sampled a set of 15 queries from the test set of the ANTIQUE dataset. Each row of the input file contains the following information:\n",
        "\n",
        "*queryid query_text*\n",
        "\n",
        "The id and text information is tab separated. queryid is a unique identifier for a query and query text has been pre-processed to remove punctutation, tokenised and stemmed using the Krovetz stemmer.  \n",
        "\n",
        "\n",
        "**Query Relevance (qrel) file**\n",
        "\n",
        "The qrel file contains the relevance judgements (ground truth) for the query passage combinations. Each row of the file contains the following information.\n",
        "\n",
        "*queryid topicid passageid relevance_judgement*\n",
        "\n",
        "Please note that the entries are space separated. The second column (topicid) can be ignored.\n",
        "\n",
        "Given below are a couple of rows of a sample qrel file.\n",
        "\n",
        "*2146313 U0 2146313_0 4*\n",
        "\n",
        "*2146313 Q0 2146313_23 2*\n",
        "\n",
        "The relevance judgements range from values 1-4. The description of the labels is given below:\n",
        "\n",
        "Label 1: Non-Relevant\n",
        "\n",
        "Label 2: Slightly Relevant\n",
        "\n",
        "Label 3 : Relevant\n",
        "\n",
        "Label 4: Highly Relevant\n",
        "\n",
        "Note: that for metrics with binary relevance assumptions, Labels 1 and 2 are considered non-relevant and Labels 3 and 4 are considered relevant.\n",
        "\n",
        "Note: if a query-document pair is not listed in the qrels file, we assume that the document is not relevant to the query.\n",
        "\n",
        "\n",
        "**Collection file**\n",
        "\n",
        "Each row of the file consists of the following information:\n",
        "\n",
        "*passage_id  passage_text*\n",
        "\n",
        "The id and text information is tab separated. The passage text has been pre-processed to remove punctutation, tokenised and stemmed using the Krovetz stemmer (same as queries). The terms in the passage text can be accessed by splitting the text based on space.\n",
        "\n",
        "\n",
        "In this section, you have to implement the following: \n",
        "\n",
        "* Load the queries from the query file into a datastructure\n",
        "* Load the query relevance information into datastructure. You can reuse some of the code written in Assignment 1 for this and make modifications to it as needed.\n",
        "\n",
        "\n",
        "You can use any additional datastructures than the suggested ones for your implementation. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDsPg_A8oZlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece03503-d24c-46dc-c0df-2719d5d1e359"
      },
      "source": [
        "import pandas as pd\n",
        "from math import log\n",
        "from collections import Counter, defaultdict\n",
        "''' \n",
        "This function is used to load query file information into datastructure(s).\n",
        "Return Variables: \n",
        "queries - mapping from queryid to querytext\n",
        "'''\n",
        "def loadQueries(queries_file):\n",
        "    #enter your code here \n",
        "    with open(queries_file) as f:  \n",
        "      qid = []\n",
        "      qtext = [] \n",
        "      qdlist = []\n",
        "      for line in f:\n",
        "        temp = line.split(\"\\t\")\n",
        "        qid.append(temp[0])\n",
        "        qtext.append(temp[1].strip('\\n'))\n",
        "        qlist = temp[1].strip('\\n').split()\n",
        "\n",
        "      queries = pd.DataFrame({'query_id':qid, 'query_txt':qtext})\n",
        "    \n",
        "    return queries\n",
        "\n",
        "\n",
        "'''\n",
        "This function is used to load qrel file information into datastructure(s).\n",
        "The qrel file format is the same as the one provided in Assignment 1 and is given below:\n",
        "\"queryid topicid passageid relevance_judgement\"\n",
        "The entries are space separated. \n",
        "You can copy your qrel loading code from Assignment 1 and make modifications if necessary.\n",
        "\n",
        "Return Variables:\n",
        "num_queries - number of queries in the qrel file\n",
        "qrels - query relevance information \n",
        "'''\n",
        "def loadQrels(qrel_file):\n",
        "     #enter your code here \n",
        "     qid = []\n",
        "     pid =[]\n",
        "     tup = []\n",
        "     reljudg = []\n",
        "     qrels = defaultdict(lambda : '')\n",
        "     with open(qrel_file) as f:   \n",
        "       for line in f:\n",
        "         temp = line.split(\" \")\n",
        "         tup = (temp[0], temp[2])\n",
        "         qrels[tup] = int(temp[3].strip('\\n'))\n",
        "\n",
        "     num_queries = len(qrels)\n",
        "\n",
        "     return num_queries,qrels\n",
        "\n",
        "# You can return additional datastructures for your implementation.     \n",
        "queries = loadQueries(queries_file)\n",
        "num_queries, qrels = loadQrels(qrel_file)\n",
        "\n",
        "\n",
        "print ('Total Num of queries in the query file  : {0}'.format(len(queries)))\n",
        "print ('Total Num of queries in the qrel file  : {0}'.format(num_queries))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Num of queries in the query file  : 15\n",
            "Total Num of queries in the qrel file  : 456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5LNkC_Nz1j"
      },
      "source": [
        "\n",
        "In the cell below, an inverted index with count has been created in memory. Please run the cell and use the variables for implementing the retrieval models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGIELwv9N7WI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e17e6a-e20c-4ae5-9918-4cdc1d2247bc"
      },
      "source": [
        "'''\n",
        "An inverted index with count information. \n",
        "'''\n",
        "class indexCount:\n",
        "   pcount = 0 \n",
        "   ctf = {}\n",
        "   sumdl = 0\n",
        "   avgdl = 0  \n",
        "   doclen = {}\n",
        "   index = {}\n",
        "   probctf = {}\n",
        "   \n",
        "   \n",
        "   def __init__(self, col):\n",
        "     self.col = col\n",
        "     \n",
        "   \n",
        "   def create_index(self):\n",
        "     for line in open(self.col):    \n",
        "       pid,ptext = line.strip().split('\\t')\n",
        "       self.pcount+=1 \n",
        "       \n",
        "       if pid not in self.doclen:\n",
        "           self.doclen[pid]=0\n",
        "       pfreq = {}\n",
        "       for term in ptext.split(' '):\n",
        "           self.sumdl+=1\n",
        "           if term not in self.ctf:\n",
        "             self.ctf[term]=0\n",
        "           self.ctf[term]+=1 \n",
        "           self.doclen[pid]+=1 \n",
        "           \n",
        "           if term not in pfreq:\n",
        "              pfreq[term]=0\n",
        "           pfreq[term]+=1\n",
        "       \n",
        "       \n",
        "       for k,v in pfreq.items():\n",
        "        if k not in self.index:\n",
        "          self.index[k]=[]      \n",
        "        self.index[k].append(pid+\":\"+str(v)) \n",
        "\n",
        "     for k,v in self.ctf.items():\n",
        "        self.probctf[k]=v/float(self.sumdl) \n",
        "\n",
        "     self.avgdl = self.sumdl/float(self.pcount)\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "buildIndex = indexCount(col)      \n",
        "buildIndex.create_index()\n",
        "\n",
        "\n",
        "''' \n",
        "inverted index with count: dict with term as key and posting list as value\n",
        "posting list is a list with each element in the format \"passage_id:term frequency\"\n",
        "Example - {'the': ['2020338_0:11', '3174498_1:4']} \n",
        "'''  \n",
        "index = buildIndex.index\n",
        "\n",
        "#total number of passages in the collection \n",
        "num_passages = buildIndex.pcount\n",
        "\n",
        "# Average passage length\n",
        "avgdl = buildIndex.avgdl\n",
        "\n",
        "# Collection Term Frequency : dict with term as the key and the term frequency in collection as value\n",
        "ctf = buildIndex.ctf\n",
        "\n",
        "# Probability Term Frequencies : dict with terms as key and probability distribution over term frequencies as value\n",
        "probctf = buildIndex.probctf\n",
        "\n",
        "#dict with passageId as key and number of tokens in the passage as value  \n",
        "doclen = buildIndex.doclen\n",
        "\n",
        "# Total number of tokens in the collection\n",
        "totNumTerms = buildIndex.sumdl\n",
        "\n",
        " \n",
        "print ('Total number of passages in the collection :{0}'.format(num_passages))\n",
        "print('Average passage length :{0}'.format(avgdl))\n",
        "print('Total num of unique terms :{0}'.format(len(ctf)))\n",
        "print('Total num of terms in the collection :{0}'.format(totNumTerms))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of passages in the collection :403492\n",
            "Average passage length :41.11619809066846\n",
            "Total num of unique terms :149467\n",
            "Total num of terms in the collection :16590057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVRaVZJNq6_1"
      },
      "source": [
        "# 2 : Vector Space model (VSM model) (25 points)\n",
        "\n",
        "In the cell below, implement the VSM model given in Slide 19 of 'Basic Retrieval Models Part 1'. The score function has been given below for reference.\n",
        "\n",
        "$$ score(q,p) = \\sum_{w \\in {q \\cap p}} count(w,q) \\frac{ln(1+ln(1+count(w,p)))}{1-b+b \\frac{|p|}{avgdl}} ln \\frac{|C|+1}{df(w)} $$\n",
        "\n",
        "$score(q,p)$ - score assigned to a passage $p$ for a query $q$\n",
        "\n",
        "$count(w,q)$ - number of times term $w$ occurs in query $q$\n",
        "\n",
        "$count(w,p)$ - number of times term $w$ occurs in passage $p$ \n",
        "\n",
        "$b$ - set this to 0.75\n",
        "\n",
        "$|p|$ - Number of tokens in passage $p$\n",
        "\n",
        "$avgdl$ - Average number of tokens in passages in collection\n",
        "\n",
        "$|C|$ - number of passages in collection $C$\n",
        "\n",
        "$df(w)$ - number of passages containing term $w$\n",
        "\n",
        "Please note that we consider each query term once, since this is equivalent to a dot product. \n",
        "\n",
        "For each query, you have to return the top 5 retrieved passages ranked based on the score returned by the VSM model using \"term at a time\" scoring method. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE70hJ44Xm09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c507871-f4cd-456a-d852-ef9d0ed4a9b3"
      },
      "source": [
        "'''\n",
        "Rank passages for each query and return top 5 passages. \n",
        "Return Variables:\n",
        "final_ranking_vsm : map with query id as key and list of top 5 ranked passages as value                \n",
        "'''\n",
        "def vsm(queries, index, avgdl, num_passages, doclen):\n",
        "   #enter your code here\n",
        "   b = 0.75\n",
        "   res_dict = {}\n",
        "   final_scores = defaultdict(lambda : defaultdict(lambda: 0)) #setting default score to be zero\n",
        "   final_ranking_vsm = {}\n",
        "\n",
        "   for ind, row in queries.iterrows():\n",
        "     qid = row['query_id']\n",
        "     qterms = Counter(row['query_txt'].split())\n",
        "     for qt, qtcount in qterms.items():\n",
        "       dfw = len(index[qt])\n",
        "       postlist = index[qt]\n",
        "       hscore = 0\n",
        "       for pas in postlist:\n",
        "         passplit = pas.split(\":\")\n",
        "         #print(passplit)\n",
        "         pid = passplit[0]\n",
        "         countWP = int(passplit[1])\n",
        "         #score calculation\n",
        "         ptoken = doclen[passplit[0]]\n",
        "         t1 = log(1 + log(1 + countWP))\n",
        "         t2 = 1 - b + ((ptoken * b)/avgdl)\n",
        "         t3 = log((num_passages + 1)/dfw)\n",
        "         hscore = (qtcount * (t1/t2) * t3) #for one term in query with passage\n",
        "         #updating the final score with this\n",
        "         final_scores[qid][pid] += hscore\n",
        "      \n",
        "     final_ranking_vsm[qid] = sorted(final_scores[qid].items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "   \n",
        "   return final_ranking_vsm\n",
        "\n",
        "final_ranking_vsm = vsm(queries, index, avgdl, num_passages, doclen)\n",
        "\n",
        "# Hint: The score would be in the interval: [13,14]\n",
        "print ('The top retrieved passage and score for query id \"3698636\" using VSM is : {0}'.format(final_ranking_vsm['3698636'][:1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top retrieved passage and score for query id \"3698636\" using VSM is : [('754739_3', 13.046982491248508)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaaHsH4IOuJq"
      },
      "source": [
        "# 3: BM25 (25 points)\n",
        "\n",
        "In the cell below, implement the BM25 model given in slide 31 of 'Basic Retrieval Models Part 3'.\n",
        "\n",
        "$$score(q,p) = \\sum_{w \\in {q \\cap p}} \\frac{(k_1+1) count(w,p)}{k_1(1-b+b(\\frac{|p|}{avgdl})) + count(w,p)} ln\\frac{|C|-df(w)+0.5}{df(w)+0.5}$$\n",
        "\n",
        "\n",
        "$score(q,p)$ - score assigned to a passage $p$ for a query $q$\n",
        "\n",
        "$count(w,p)$ - number of times term $w$ occurs in passage $p$ \n",
        "\n",
        "$b$ - set this to 0.75\n",
        "\n",
        "$|p|$ - Number of tokens in passage $p$\n",
        "\n",
        "$avgdl$ - Average number of tokens in passages in collection\n",
        "\n",
        "$|C|$ - number of passages in collection $C$\n",
        "\n",
        "$df(w)$ - number of passages containing term $w$\n",
        "\n",
        "$k_1$ - set to 1.2 \n",
        "\n",
        "Please note that we iterate over all query tokens including repetitions. \n",
        "\n",
        "Similar to the previous model, return the top 5 retrieved passages for each query ranked based on the BM25 scoring using \"term at a time\" scoring method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KX-4F06d8Pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea79061-32b5-41d5-fee0-454a1359b763"
      },
      "source": [
        "'''\n",
        "Rank passages for each query and return top 5 passages. \n",
        "Return Variables:\n",
        "final_ranking_bm25 : map with query id as key and list of top 5 ranked passages as value                \n",
        "'''\n",
        "def bm25(queries, index, avgdl, num_passages, doclen):\n",
        "   #enter your code here\n",
        "   b = 0.75\n",
        "   k1 = 1.2\n",
        "   res_dict = {}\n",
        "   final_scores = defaultdict(lambda : defaultdict(lambda: 0)) #setting default score to be zero\n",
        "   final_ranking_bm25 = {}\n",
        "\n",
        "   for ind, row in queries.iterrows():\n",
        "     qid = row['query_id']\n",
        "     qterms = Counter(row['query_txt'].split())\n",
        "     for qt, qtcount in qterms.items():\n",
        "       dfw = len(index[qt])\n",
        "       postlist = index[qt]\n",
        "       hscore = 0\n",
        "       for pas in postlist:\n",
        "         passplit = pas.split(\":\")\n",
        "         #print(passplit)\n",
        "         pid = passplit[0]\n",
        "         countWP = int(passplit[1])\n",
        "         #score calculation\n",
        "         ptoken = doclen[passplit[0]]\n",
        "         num = (k1+1) * countWP\n",
        "         t1 = 1 - b + ((ptoken * b)/avgdl)\n",
        "         den = (k1 * t1) + countWP\n",
        "         t2 = log((num_passages - dfw + 0.5) / (dfw + 0.5))\n",
        "         hscore = (num/den) * t2  #for one term in query with passage\n",
        "         #updating the final score with this\n",
        "         final_scores[qid][pid] += hscore\n",
        "      \n",
        "     final_ranking_bm25[qid] = sorted(final_scores[qid].items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "   \n",
        "\n",
        "   return final_ranking_bm25\n",
        "\n",
        "\n",
        "final_ranking_bm25 = bm25(queries, index, avgdl, num_passages, doclen)\n",
        "\n",
        "# Hint: The score would be in the interval: [18,19]\n",
        "print ('The top retrieved passage and score for query id \"3698636\" using BM25 is : {0}'.format(final_ranking_bm25['3698636'][:1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top retrieved passage and score for query id \"3698636\" using BM25 is : [('3698636_9', 18.355784008150206)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeYoIFeAd-K_"
      },
      "source": [
        "# 4: Query Likelihood Model (20 points)\n",
        "\n",
        "In the cell below, implement the Query  Likelihood model using the Jelinek-Mercer (JM) Smoothing technique. The score can be computed as follows:\n",
        "\n",
        "   \n",
        "   $$score(q,p) = \\sum_{w \\in {q}} ln( (1-\\lambda) P_{MLE}(w|p) + \\lambda P_{MLE}(w|C))$$ \n",
        "\n",
        "$score(q,p)$ - score assigned to a passage $p$ for a query $q$\n",
        "\n",
        " $P_{MLE}(w|p)$ = $\\frac{count(w,p)}{|p|}$ \n",
        "\n",
        " $P_{MLE}(w|C)$ = $\\frac{count(w,C)}{N}$\n",
        "\n",
        "$count(w,p)$ - number of times term $w$ occurs in passage $p$ \n",
        "\n",
        "$count(w,C)$ - number of times term $w$ occurs in collection $C$\n",
        "\n",
        "$|p|$ - Number of tokens in passage $p$\n",
        "\n",
        "\n",
        "$N$ - Number of tokens in collection $C$\n",
        "\n",
        "$\\lambda$ - set to 0.2\n",
        "\n",
        "Please note that we iterate over all query tokens including repetitions. \n",
        "\n",
        "Similar to the previous model, return the top 5 retrieved passages for each query ranked based on the QL scoring using \"term at a time\" scoring method. . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8b2JmueEx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ca61fb9-f11d-4d55-ec65-b4d6298a395a"
      },
      "source": [
        "'''\n",
        "Rank passages for each query and return top 5 passages. \n",
        "Return Variables:\n",
        "final_ranking_ql_jm: map with query id as key and list of top 5 ranked passages as value           \n",
        "'''\n",
        "def ql_jm(queries, index, doclen, probctf):\n",
        "   #enter your code here\n",
        "   lmb = 0.2\n",
        "   final_scores = defaultdict(lambda : defaultdict(lambda: 0)) #setting default score to be zero\n",
        "   totplist = list(doclen.keys())\n",
        "   final_ranking_ql_jm = {}\n",
        "\n",
        "   for ind, row in queries.iterrows():\n",
        "        qid = row['query_id']\n",
        "        qterms = Counter(row['query_txt'].split())\n",
        "        for qt, qtcount in qterms.items():\n",
        "          postlist = index[qt]\n",
        "          hscore = 0\n",
        "          pwc = ctf[qt]/totNumTerms\n",
        "          tplist = []\n",
        "          for pas in postlist:\n",
        "            passplit = pas.split(\":\")\n",
        "            pid = passplit[0]\n",
        "            countWP = int(passplit[1])\n",
        "            #score calculation\n",
        "            ptoken = doclen[passplit[0]]\n",
        "            pwp = countWP/ptoken\n",
        "            hscore = log(((1 - lmb) * pwp) + (lmb * pwc))\n",
        "            #updating the final score with this\n",
        "            final_scores[qid][pid] += hscore\n",
        "            tplist.append(pid)\n",
        "          rem_pid = list(set(totplist) - set(tplist))\n",
        "\n",
        "          for pid in rem_pid:\n",
        "            final_scores[qid][pid] += log(lmb * pwc)\n",
        "\n",
        "        final_ranking_ql_jm[qid] = sorted(final_scores[qid].items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "   \n",
        "\n",
        "   return final_ranking_ql_jm\n",
        "\n",
        "final_ranking_ql_jm = ql_jm(queries, index, doclen, probctf)\n",
        "\n",
        "# Hint: The score value would in the interval [-20,-19]\n",
        "print ('The top retrieved passage and score for query id \"3698636\" using Jelinek-Mercer smoothing is : {0}'.format(final_ranking_ql_jm['3698636'][:1]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The top retrieved passage and score for query id \"3698636\" using Jelinek-Mercer smoothing is : [('3698636_9', -19.357461855563887)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aihw-JneGVO"
      },
      "source": [
        "# 5: Evaluation (20 points)\n",
        "\n",
        "In the cell, evaluate the top 5 retrieved passages coresponding to each of the models using Precision@5 and Recall@5 metrics.\n",
        "You can use the code from assignment 1 modified as needed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jnR4EmneKOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c785d4ed-65c9-4694-9753-46a1a2d81637"
      },
      "source": [
        "\n",
        "# return precision of top 5 retrieved passages\n",
        "def calcPrecision(top, qrels, rank_in):\n",
        "     #enter your code here \n",
        "     net = 0 \n",
        "     for key, val in rank_in.items():\n",
        "       qid = key\n",
        "       cnt = 0\n",
        "       for j in val:\n",
        "         if ((qid, j[0])) in qrels:\n",
        "           if qrels[(qid, j[0])] >= 3:\n",
        "             cnt += 1\n",
        "       prec = (cnt/top)\n",
        "       net += prec\n",
        "    \n",
        "     return net/len(rank_in)\n",
        "\n",
        "# return recall of top 5 retrieved passages\n",
        "def calcRecall(top, qrels, rank_in):\n",
        "    #enter your code here \n",
        "    net = 0\n",
        "    for key, val in rank_in.items():\n",
        "       qid = key\n",
        "       cnt = 0\n",
        "       recos = len(list(k for k, v in qrels.items() if qrels[k] >= 3 and k[0] == qid))\n",
        "       for j in val:\n",
        "         if ((qid, j[0])) in qrels:\n",
        "           if qrels[(qid, j[0])] >= 3:\n",
        "             cnt += 1\n",
        "       rec = (cnt/recos)\n",
        "       net += rec\n",
        "  \n",
        "    return net/len(rank_in)   \n",
        "\n",
        "# Hint: Precision value interval [0.1,0.2],  Recall value interval [0.04,0.05]\n",
        "print(\"Evaluate VSM model\")\n",
        "print ('Precision at top 5 : {0}'.format(calcPrecision(5, qrels, final_ranking_vsm)))\n",
        "print ('Recall at top 5 : {0}'.format(calcRecall(5, qrels, final_ranking_vsm)))\n",
        "print (\"*********************************************************************\")\n",
        "# Hint: Precision value interval [0.3,0.4], Recall value interval [0.10,0.20]\n",
        "print(\"Evaluate BM25 model\")\n",
        "print ('Precision at top 5 : {0}'.format(calcPrecision(5, qrels, final_ranking_bm25)))\n",
        "print ('Recall at top 5 : {0}'.format(calcRecall(5, qrels, final_ranking_bm25)))\n",
        "print (\"*********************************************************************\")\n",
        "# Hint: Precision value interval [0.3,0.4], Recall value interval [0.1,0.2]\n",
        "print(\"Evaluate QL model with Jelinek-Mercer smoothing\")\n",
        "print ('Precision at top 5 : {0}'.format(calcPrecision(5, qrels, final_ranking_ql_jm)))\n",
        "print ('Recall at top 5 : {0}'.format(calcRecall(5, qrels, final_ranking_ql_jm)))\n",
        "print (\"*********************************************************************\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate VSM model\n",
            "Precision at top 5 : 0.16\n",
            "Recall at top 5 : 0.04649035880919939\n",
            "*********************************************************************\n",
            "Evaluate BM25 model\n",
            "Precision at top 5 : 0.36000000000000004\n",
            "Recall at top 5 : 0.12100747347124159\n",
            "*********************************************************************\n",
            "Evaluate QL model with Jelinek-Mercer smoothing\n",
            "Precision at top 5 : 0.33333333333333337\n",
            "Recall at top 5 : 0.11713800960177773\n",
            "*********************************************************************\n"
          ]
        }
      ]
    }
  ]
}
